# (Virtual) Machine Learning / Data Science Paper Club 
A repository of papers accompanying EF's data science community's paper club.

## Joining Instructions

The paper club is hosted bi-weekly on Microsoft teams on Tuesday or Thursday 4-5pm GMT. Note that the presentation part will be recorded and shared afterwards but the discussions won't be. Ask Billy for the invite.

## How the paper club works
The format of the paper club will be one of the community members will do a short 10-15 min presentation of the paper being discussed and then we open up for discussions afterwards. The whole meeting will last for an hour max.

During the presentation:
* Feel free to make comments in the chat about the paper being discussed

After the presentation and during the discussion:
* Raise your (virtual) hand if you want to speak and the moderator will ask you to unmute and go from there.
* Questions are not only directed at the presenter but everyone to chime in on their particular thoughts about the paper.

Rota of the paper:
* We will take recommendations from the community via Teams/Github issues or PRs and then we will update our paper backlog.
* One person will volunteer to take on one of the papers from the backlog each week and the paper will be posted on here and Teams at least one week before the meeting time.

## Next meetup's paper
* [25/03/2021] TBC

## Paper backlog
* placeholder

## Paper history
* placeholder

## Supplementary material

For those new to machine learning, these are some recommended reading material:

- Goodfellow, I., Bengio, Y., & Courville, A. (2016). [Deep learning.](http://www.deeplearningbook.org/) MIT press.

- Goldberg, Y. (2016). [A primer on neural network models for natural language processing.](http://u.cs.biu.ac.il/~yogo/nnlp.pdf) Journal of Artificial Intelligence Research, 57, 345-420.

- Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., & Yu, P. S. (2019). [A comprehensive survey on graph neural networks.](https://arxiv.org/pdf/1901.00596.pdf) arXiv preprint arXiv:1901.00596.

- Provost, F. and Fawcett, T. (2013). [Data science for business.](https://www.amazon.co.uk/Data-Science-Business-data-analytic-thinking/dp/1449361323) Sebastopol: O'Reilly.

Transformer-related resources:

- [The illustrated transformer](http://jalammar.github.io/illustrated-transformer/)

- [The annotated transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html#attention)

- [Embedding layers in BERT, explained](https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a)

- Examples of BERT: [sentiment analysis](https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb) and [feature extraction from BERT](https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b)

Feel free to suggest and share more here!
